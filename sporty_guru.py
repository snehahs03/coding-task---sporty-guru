# -*- coding: utf-8 -*-
"""sporty_guru.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ph3PqXIWzKWucn6AUB2H3x69t6qGMx8e
"""

#Load all dependecies
import numpy as np
import tensorflow as tf
import keras
import keras.datasets
from keras.models import Sequential
from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.utils import to_categorical
from keras.optimizers import Adam
from keras.callbacks import ReduceLROnPlateau, EarlyStopping
from keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt

bs = 32 #batch size
ne = 50 #number of epochs

cifarc10 = tf.keras.datasets.cifar10.load_data()

def prep_pixels(train, test):
	# convert from integers to floats
	train_norm = train.astype('float32')
	test_norm = test.astype('float32')
	# normalize to range 0-1
	train_norm = train_norm / 255.0
	test_norm = test_norm / 255.0
	# return normalized images
	return train_norm, test_norm

# load train and test dataset
#def load_dataset():
	# load dataset
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
	# one hot encode target values
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)
#return x_train, y_train, x_test, y_test

x_train, x_test = prep_pixels(x_train, x_test)

print(x_train.shape, y_train.shape)

"""NET A"""

#Neural network
model1 = Sequential()
model1.add(Flatten())
model1.add(Dense(10))

model1.summary()

optimizer = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
model1.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])

learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=1, factor=0.5, 
                                            min_lr=0.1, cooldown=2)

train_datagen = ImageDataGenerator(rotation_range=60, width_shift_range=0.2, height_shift_range=0.2,
                                   shear_range=0.2, zoom_range=0.2, fill_mode='nearest')

train_datagen.fit(x_train)
val_datagen = ImageDataGenerator()
val_datagen.fit(x_test)

history1 = model1.fit_generator(train_datagen.flow(x_train,y_train, batch_size=bs),
                              epochs = ne, validation_data = val_datagen.flow(x_test, y_test),
                              verbose = 1, steps_per_epoch=(x_train.shape[0] // bs), 
                              validation_steps=(x_test.shape[0] // bs))

model1.save("netA.h5")

print(history1.history.keys())

#plot acc and test
plt.plot(history1.history['accuracy'])
plt.plot(history1.history['val_accuracy'])
plt.title('NET A accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()
plt.plot(history1.history['loss']) 
plt.plot(history1.history['val_loss']) 
plt.title('NET A loss') 
plt.ylabel('Loss') 
plt.xlabel('Epoch') 
plt.legend(['Train', 'Test'], loc='upper left') 
plt.show()

"""NET B"""

#Neural network
model2 = Sequential()
model2.add(Flatten())
model2.add(Dense(300, activation='relu' ))
model2.add(Dense(10))

model2.summary()

#optimizer = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False, )
model2.compile(loss='categorical_crossentropy', optimizer="rmsprop", metrics=['accuracy'])

learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=1, factor=0.5, 
                                            min_lr=0.0001, cooldown=2)

train_datagen = ImageDataGenerator(rotation_range=60, width_shift_range=0.2, height_shift_range=0.2,
                                   shear_range=0.2, zoom_range=0.2, fill_mode='nearest')

train_datagen.fit(x_train)
val_datagen = ImageDataGenerator()
val_datagen.fit(x_test)

history2 = model2.fit_generator(train_datagen.flow(x_train,y_train, batch_size=bs),
                              epochs = ne, validation_data = val_datagen.flow(x_test, y_test),
                              verbose = 1, steps_per_epoch=(x_train.shape[0] // bs), 
                              validation_steps=(x_test.shape[0] // bs))

model2.save("netB.h5")

#plot acc and test
plt.plot(history2.history['accuracy'])
plt.plot(history2.history['val_accuracy'])
plt.title('NET B accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()
plt.plot(history2.history['loss']) 
plt.plot(history2.history['val_loss']) 
plt.title('NET B loss') 
plt.ylabel('Loss') 
plt.xlabel('Epoch') 
plt.legend(['Train', 'Test'], loc='upper left') 
plt.show()

"""NET C"""

#Neural network
model3 = Sequential()
model3.add(Conv2D(25, kernel_size=5, activation='relu', input_shape=(32,32,3)))
model3.add(MaxPooling2D((2,2), strides=(2,2)))
model3.add(Flatten())
model3.add(Dense(10))

model3.summary()

model3.compile(loss='categorical_crossentropy', optimizer="Adam", metrics=['accuracy'])

history3 = model3.fit_generator(train_datagen.flow(x_train,y_train, batch_size=bs),
                              epochs = ne, validation_data = val_datagen.flow(x_test, y_test),
                              verbose = 1, steps_per_epoch=(x_train.shape[0] // bs), 
                              validation_steps=(x_test.shape[0] // bs))

model3.save("netC.h5")

#plot acc and test
plt.plot(history3.history['accuracy'])
plt.plot(history3.history['val_accuracy'])
plt.title('NET C accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()
plt.plot(history3.history['loss']) 
plt.plot(history3.history['val_loss']) 
plt.title('NET C loss') 
plt.ylabel('Loss') 
plt.xlabel('Epoch') 
plt.legend(['Train', 'Test'], loc='upper left') 
plt.show()

print("Training accuracy of Net A is 11.3% and test accuracy is 11.1%" '\n'
      "Training accuracy of Net B is 12.18% and test accuracy is 12.7%" '\n' "Training accuracy of Net C is 9.8% and test accuracy is 9.8%")

acc1_1 = np.array(history1.history['accuracy'])
acc1_2 = np.array(history1.history['val_accuracy'])
acc2_1 = np.array(history2.history['accuracy'])
acc2_2 = np.array(history2.history['val_accuracy'])
acc3_1 = np.array(history3.history['accuracy'])
acc3_2 = np.array(history3.history['val_accuracy'])

epochs = range(len(acc2_1))

plt.figure(figsize=(10,10), dpi=70)


# Plot training and validation accuracy per epoch
plt.plot(epochs, acc1_1, "-b", label = "netA_train", )
plt.plot(epochs, acc1_2, "-b", linestyle='dashed', label = "netA_test", )
plt.plot(epochs, acc2_1, "-g", label = "netB_train", )
plt.plot(epochs, acc2_2, "-g",linestyle='dashed', label = "netB_test", )
plt.plot(epochs, acc3_1, "-y", label = "netC_train", )
plt.plot(epochs, acc3_2, "-y",linestyle='dashed', label = "netC_test", )
plt.legend(loc="lower right")
plt.title('Accuracy histories')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.show()

